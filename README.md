Transformer from Scratch
I am a theoretical physicist exploring modern AI architectures through first-principles understanding.
This repository follows the superb lecture by Andrej Karpathy (https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5065s), where I build a Transformer model from scratch â€” line by line â€” to internalize the self-attention mechanism in full detail and connect it to my physics intuition whenever possible.


ðŸ“š Foundational Readings
While training neural networks, I found the following works particularly illuminating:
1. Deep Residual Learning for Image Recognition (He et al., 2015, https://arxiv.org/abs/1512.03385)
Key lesson: naively, one expects that a deeper NN with more model parameters would result in overfitting. This is not what is observed in practice. In practice, deeper NNs are more difficult to train, and the proposed residual architecture efficiently solves the training issue.
