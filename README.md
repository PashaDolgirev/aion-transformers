Transformer from Scratch

I am a theoretical physicist exploring modern AI architectures through first-principles understanding.
This repository follows the superb lecture by Andrej Karpathy (https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5065s), where I build a Transformer model from scratch â€” line by line â€” to internalize the self-attention mechanism in full detail and connect it to my physics intuition whenever possible.


ðŸ“š Foundational Readings

While training neural networks, I found the following works particularly illuminating:
1. Deep Residual Learning for Image Recognition (He et al., 2015, https://arxiv.org/abs/1512.03385). Key lesson: naively, one expects that a deeper NN with more model parameters would result in overfitting. This is not what is observed in practice. In practice, deeper NNs are more difficult to train, and the proposed residual architecture efficiently solves the training issue.
2. Attention Is All You Need â€” Vaswani et al., 2017, https://arxiv.org/abs/1706.03762. The original breakthrough paper introducing the Transformer architecture, based on the (self) attention mechanism.
3. 3Blue1Brown: Transformer Series â€” Chapters 5â€“7, https://www.youtube.com/watch?v=wjZofJX0v4M. A beautifully intuitive explanation of the self-attention mechanism, really complements the Attention Is All You Need paper.
4. Murphy, Probabilistic Machine Learning: An Introduction â€” Chapters 13â€“15: concise and complete overview of deep learning fundamentals, love the rigor level of the book.
