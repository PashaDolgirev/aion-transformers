I am a theoretical phycisist, and here I follow the superb lecture by A. Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5065s.
My goal is to dive into the self-attention mechanism, in every possible detail and connect to my physics intuition whenever possible.

In training a NN, I found these papers extremely useful:
1. Residual NNs: https://arxiv.org/abs/1512.03385. Key lesson: naively, one expects that a deeper NN one has with more model parameters should result in overfitting. This is not what is observed in practice. In practice, deeper NN are more difficult to train, and the residual architecture is a way to make training of deeper NN efficient.
